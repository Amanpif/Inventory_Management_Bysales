{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb7cd8e-fd7a-4d33-8f5b-921c2f52331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTING MODULES\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117f1753-5467-434e-bbe9-cdf3896a2a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING DATASETS\n",
    "sales_data=pd.read_csv('features.csv',low_memory=False)\n",
    "sample_submission=pd.read_csv('sampleSubmission.csv')\n",
    "stores_data=pd.read_csv('stores.csv',low_memory=False)\n",
    "train_data=pd.read_csv('train.csv',low_memory=False)\n",
    "test_data=pd.read_csv('test.csv',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad80fa23-3941-4263-85f4-0353a148d45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MERGING THE DATASETS\n",
    "sales_data_merged=pd.merge(sales_data,stores_data,on=\"Store\",how=\"inner\")\n",
    "sales_data_merged_final=pd.merge(sales_data_merged,train_data,on=[\"Store\",\"Date\"],how=\"inner\")\n",
    "df=sales_data_merged_final\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ad3a95-7f9b-408a-89ee-b1f1bc55e661",
   "metadata": {},
   "source": [
    "## DATA PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7c3856-b7a0-4831-af41-7ddcde43c6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial manipulation\n",
    "df=df.drop(['IsHoliday_y'],axis=1)\n",
    "df=df.rename(columns={'IsHoliday_x':'IsHoliday'})\n",
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "df['IsHoliday']=df['IsHoliday'].astype('int32')\n",
    "df['Type'] = df['Type'].replace({'A': 0, 'B': 1,'C':2})\n",
    "df = df.astype({\n",
    "    'Store': 'int32',\n",
    "    'Temperature': 'float32',\n",
    "    'Fuel_Price': 'float32',\n",
    "    'MarkDown1': 'float32',\n",
    "    'MarkDown2': 'float32',\n",
    "    'MarkDown3': 'float32',\n",
    "    'MarkDown4': 'float32',\n",
    "    'MarkDown5': 'float32',\n",
    "    'CPI':'float32',\n",
    "    'Unemployment':'float32',\n",
    "    'Size':'int32',\n",
    "    'Dept':'int32',\n",
    "    'Weekly_Sales':'int32'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6081976-f39e-4a86-bc20-f89732f23576",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA CLEANING\n",
    "df=df.fillna(0)\n",
    "df['Date']=pd.to_datetime(df['Date'])\n",
    "df['Month']=df['Date'].dt.month\n",
    "df['Year']=df['Date'].dt.year\n",
    "df['Day']=df['Date'].dt.day\n",
    "df.drop(['Date'],axis=1,inplace=True)\n",
    "\n",
    "#Creating main event column\n",
    "main_events=[(25,11),(26,11),(24,12),(23,12)]\n",
    "\n",
    "# Create (Day, Month) tuple column\n",
    "df['day_month_tuple'] = list(zip(df['Day'], df['Month']))\n",
    "\n",
    "# Flag rows where the date matches any event\n",
    "df['is_main_event'] = df['day_month_tuple'].isin(main_events).astype(int)\n",
    "df.drop(['day_month_tuple'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ef234-4e58-4b13-8d7c-4dc874b27e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['MarkDown']=df['MarkDown1']+df['MarkDown2']+df['MarkDown4']+df['MarkDown5']\n",
    "df.drop(['MarkDown1','MarkDown2','MarkDown4','MarkDown5'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b37424-858d-4ffe-b715-e7eafd53614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f6c0db-a8f9-4594-ba73-9c01e20b9f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f193884-7e65-4052-8191-c7814d6da8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sequential split: Train on data before 2012, test on 2012\n",
    "train_df = df[df['Year'] < 2012]  # Training data (before 2012)\n",
    "test_df = df[df['Year'] == 2012]  # Test data (2012)\n",
    "\n",
    "# Sort the data by Store, Dept, and Date (now using Year, Month, Day)\n",
    "train_df = train_df.sort_values(['Store', 'Dept', 'Year', 'Month', 'Day'])\n",
    "test_df = test_df.sort_values(['Store', 'Dept', 'Year', 'Month', 'Day'])\n",
    "\n",
    "# Create lag features for the training data (no future leakage)\n",
    "train_df['Lag_1'] = train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "train_df['Lag_2'] = train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
    "train_df['Rolling_Mean_4'] = train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).mean()\n",
    "train_df['Rolling_Std_4'] = train_df.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).std()\n",
    "\n",
    "# Now prepare the test data with lag features based on training data\n",
    "history = train_df.groupby(['Store', 'Dept']).tail(4)  # Keep the last 4 weeks from training data\n",
    "test_prep = pd.concat([history, test_df]).sort_values(['Store', 'Dept', 'Year', 'Month', 'Day'])\n",
    "\n",
    "# Recalculate lag features for the combined data\n",
    "test_prep['Lag_1'] = test_prep.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "test_prep['Lag_2'] = test_prep.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
    "test_prep['Rolling_Mean_4'] = test_prep.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).mean()\n",
    "test_prep['Rolling_Std_4'] = test_prep.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).std()\n",
    "\n",
    "# Filter the test data back out (keep only the 2012 rows)\n",
    "test_df = test_prep[test_prep['Year'] == 2012]\n",
    "\n",
    "# Prepare data for training and testing\n",
    "train_x = train_df.drop(['Weekly_Sales'], axis=1)\n",
    "train_y = train_df['Weekly_Sales']\n",
    "test_x = test_df.drop(['Weekly_Sales'], axis=1)\n",
    "test_y = test_df['Weekly_Sales']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2d7c5d-ad2d-423b-bbfc-de1bb8f41b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8a52b6-1ec0-4efb-adc6-2ac04f8b98a3",
   "metadata": {},
   "source": [
    "## MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58fa550-c4a9-4dbd-82df-74e84571b335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf1 = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_drop_columns=['MarkDown','Lag_1','Lag_2','Rolling_Mean_4','Rolling_Std_4','IsHoliday','Year','Fuel_Price']\n",
    "rf1.fit(train_x.drop(rf_drop_columns,axis=1),train_y)\n",
    "rf1_preds = rf1.predict(test_x.drop(rf_drop_columns,axis=1))\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "print(r2_score(test_y,rf1_preds))\n",
    "print(mean_absolute_error(rf1_preds,test_y))\n",
    "print(root_mean_squared_error(rf1_preds,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3252b4d2-9799-461c-8a68-e9bfff54be0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#XG Boost\n",
    "from xgboost import XGBRegressor\n",
    "xg1 =XGBRegressor(n_estimators=200,max_depth=6,learning_rate=0.1,gamma=0)\n",
    "xg_drop_columns=['MarkDown','Fuel_Price','Temperature','CPI','Unemployment']\n",
    "xg1.fit(train_x.drop(xg_drop_columns,axis=1),train_y)\n",
    "xg1_preds = xg1.predict(test_x.drop(xg_drop_columns,axis=1))\n",
    "print(r2_score(test_y,xg1_preds))\n",
    "print(mean_absolute_error(test_y,xg1_preds))\n",
    "print(root_mean_squared_error(test_y,xg1_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f5dc76-4bb1-4cd0-962c-eee2c9a708f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#LGBM \n",
    "from lightgbm import LGBMRegressor\n",
    "lgb1 =LGBMRegressor(num_leaves=100,n_estimators=200,min_child_samples=20,max_depth=20,learning_rate=0.05)\n",
    "lgb_drop_columns=['MarkDown','Year','IsHoliday','Type']\n",
    "lgb1.fit(train_x.drop(lgb_drop_columns,axis=1),train_y)\n",
    "lgb1_preds = lgb1.predict(test_x.drop(lgb_drop_columns,axis=1))\n",
    "print(r2_score(test_y,lgb1_preds))\n",
    "print(mean_absolute_error(test_y,lgb1_preds))\n",
    "print(root_mean_squared_error(test_y,lgb1_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22335b68-002a-4c00-9ac1-779d5df84535",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_preds=0.2*rf1_preds+0.4*xg1_preds+0.4*lgb1_preds\n",
    "print(r2_score(test_y,final_preds))\n",
    "print(mean_absolute_error(test_y,final_preds))\n",
    "print(root_mean_squared_error(test_y,final_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7c5194-c769-40e0-9cea-dbc405c9ab07",
   "metadata": {},
   "source": [
    "## HYPERPARAMETER TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777dfd9-7319-420b-abfb-cbdd56e158d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV, TimeSeriesSplit\n",
    "from lightgbm import LGBMRegressor\n",
    "\n",
    "lgb = LGBMRegressor(random_state=42)\n",
    "lgb_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.05, 0.1, 0.3, 0.6, 0.9],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'max_depth': [-1, 10, 20],\n",
    "    'min_child_samples': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Optional: Use TimeSeriesSplit if data is time-dependent\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "\n",
    "lgb_search = RandomizedSearchCV(\n",
    "    lgb, lgb_params, n_iter=20, cv=cv, verbose=1,\n",
    "    random_state=42, n_jobs=-1, scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "lgb_search.fit(train_x.drop(lgb_drop_columns, axis=1), train_y)\n",
    "best_lgb = lgb_search.best_estimator_\n",
    "\n",
    "# Use matching features on test set\n",
    "def evaluate_model(model, test_x, test_y, name=\"Model\"):\n",
    "    preds = model.predict(test_x)\n",
    "    print(f\"\\nðŸ“Š {name} Performance on Test Set:\")\n",
    "    print(\"RÂ² Score:\", r2_score(test_y, preds))\n",
    "    print(\"MAE:\", mean_absolute_error(test_y, preds))\n",
    "    print(\"RMSE:\", root_mean_squared_error(test_y, preds))\n",
    "    print(lgb_search.best_params_)\n",
    "evaluate_model(best_lgb, test_x.drop(lgb_drop_columns, axis=1), test_y, \"LightGBM\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26a647-d1f6-4301-aeb2-7440a4cc4089",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lgb_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555aae56-8b15-478c-ae05-5d2f895d8a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define base model\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Define parameter grid\n",
    "xgb_params = {\n",
    "    'n_estimators': [150, 200, 300,400],\n",
    "    'learning_rate': [0.1, 0.3, 0.5, 0.7],\n",
    "    'max_depth': [None,3, 6, 10],\n",
    "    'gamma': [0, 0.1, 0.3]\n",
    "}\n",
    "\n",
    "# Optional: use TimeSeriesSplit if data is sequential\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb, xgb_params, n_iter=20, cv=cv, verbose=1,\n",
    "    random_state=42, n_jobs=-1, scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Fit model (make sure xg_drop_columns is defined)\n",
    "xgb_search.fit(train_x.drop(xg_drop_columns, axis=1), train_y)\n",
    "\n",
    "# Get best model\n",
    "best_xgb = xgb_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(best_xgb, test_x.drop(xg_drop_columns, axis=1), test_y, \"XGBoost\")\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\nBest Parameters for XGBoost:\")\n",
    "print(xgb_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbabd8af-9694-4b39-a03d-1e35c34b4bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define base model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Define hyperparameter grid\n",
    "rf_params = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "cv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rf_search = RandomizedSearchCV(\n",
    "    rf, rf_params, n_iter=20, cv=cv, verbose=1,\n",
    "    random_state=42, n_jobs=-1, scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Fit model (ensure rf_drop_columns is defined)\n",
    "rf_search.fit(train_x.drop(rf_drop_columns, axis=1), train_y)\n",
    "\n",
    "# Best model\n",
    "best_rf = rf_search.best_estimator_\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(best_rf, test_x.drop(rf_drop_columns, axis=1), test_y, \"Random Forest\")\n",
    "\n",
    "# Show best parameters\n",
    "print(\"\\nBest Parameters for Random Forest:\")\n",
    "print(rf_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a08e16e-8353-4240-857a-60030607da85",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0eecbd-522f-496a-afb1-defc560b9a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load all required datasets\n",
    "features = pd.read_csv('features.csv')\n",
    "stores = pd.read_csv('stores.csv')\n",
    "test = pd.read_csv('test.csv')\n",
    "\n",
    "# Merge features and stores with test\n",
    "test_merged = pd.merge(test, features, on=['Store', 'Date'], how='left')\n",
    "test_merged = pd.merge(test_merged, stores, on='Store', how='left')\n",
    "\n",
    "# Preprocessing (same as training)\n",
    "if 'IsHoliday_x' in test_merged.columns and 'IsHoliday_y' in test_merged.columns:\n",
    "    test_merged = test_merged.drop('IsHoliday_y', axis=1)\n",
    "    test_merged = test_merged.rename(columns={'IsHoliday_x': 'IsHoliday'})\n",
    "test_merged['Date'] = pd.to_datetime(test_merged['Date'])\n",
    "test_merged['IsHoliday'] = test_merged['IsHoliday'].astype('int32')\n",
    "test_merged['Type'] = test_merged['Type'].replace({'A': 0, 'B': 1, 'C': 2})\n",
    "\n",
    "# Handle datatypes\n",
    "test_merged = test_merged.astype({\n",
    "    'Store': 'int32',\n",
    "    'Temperature': 'float32',\n",
    "    'Fuel_Price': 'float32',\n",
    "    'MarkDown1': 'float32',\n",
    "    'MarkDown2': 'float32',\n",
    "    'MarkDown3': 'float32',\n",
    "    'MarkDown4': 'float32',\n",
    "    'MarkDown5': 'float32',\n",
    "    'CPI': 'float32',\n",
    "    'Unemployment': 'float32',\n",
    "    'Size': 'int32',\n",
    "    'Dept': 'int32'\n",
    "})\n",
    "test_merged.fillna(0, inplace=True)\n",
    "\n",
    "# Extract date parts\n",
    "test_merged['Month'] = test_merged['Date'].dt.month\n",
    "test_merged['Year'] = test_merged['Date'].dt.year\n",
    "test_merged['Day'] = test_merged['Date'].dt.day\n",
    "\n",
    "# Add main event flag\n",
    "main_events = [(25, 11), (26, 11), (24, 12), (23, 12)]\n",
    "test_merged['day_month_tuple'] = list(zip(test_merged['Day'], test_merged['Month']))\n",
    "test_merged['is_main_event'] = test_merged['day_month_tuple'].isin(main_events).astype(int)\n",
    "test_merged.drop(['day_month_tuple'], axis=1, inplace=True)\n",
    "\n",
    "# Recreate Date column for sorting\n",
    "test_merged['Date'] = pd.to_datetime(test_merged[['Year', 'Month', 'Day']])\n",
    "test_merged.sort_values(['Store', 'Dept', 'Date'], inplace=True)\n",
    "\n",
    "# Add lag/rolling features by merging with train data\n",
    "# Load and process train data to calculate lags\n",
    "train = pd.read_csv('train.csv')\n",
    "train['Date'] = pd.to_datetime(train['Date'])\n",
    "features['Date']=pd.to_datetime(features['Date'])\n",
    "train_full = pd.merge(train, features, on=['Store', 'Date'], how='left')\n",
    "train_full = pd.merge(train_full, stores, on='Store', how='left')\n",
    "\n",
    "# Same preprocessing on train_full\n",
    "if 'IsHoliday_x' in train_full.columns and 'IsHoliday_y' in train_full.columns:\n",
    "    train_full = train_full.drop('IsHoliday_y', axis=1)\n",
    "    train_full = train_full.rename(columns={'IsHoliday_x': 'IsHoliday'})\n",
    "train_full['IsHoliday'] = train_full['IsHoliday'].astype('int32')\n",
    "train_full['Type'] = train_full['Type'].replace({'A': 0, 'B': 1, 'C': 2})\n",
    "train_full.fillna(0, inplace=True)\n",
    "train_full['Month'] = train_full['Date'].dt.month\n",
    "train_full['Year'] = train_full['Date'].dt.year\n",
    "train_full['Day'] = train_full['Date'].dt.day\n",
    "train_full['day_month_tuple'] = list(zip(train_full['Day'], train_full['Month']))\n",
    "train_full['is_main_event'] = train_full['day_month_tuple'].isin(main_events).astype(int)\n",
    "train_full['Date'] = pd.to_datetime(train_full[['Year', 'Month', 'Day']])\n",
    "train_full.sort_values(['Store', 'Dept', 'Date'], inplace=True)\n",
    "\n",
    "# Combine train and test\n",
    "train_full['source'] = 'train'\n",
    "test_merged['source'] = 'test'\n",
    "combined = pd.concat([train_full, test_merged], sort=False)\n",
    "\n",
    "# Add lags and rolling features\n",
    "combined['Lag_1'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1)\n",
    "combined['Lag_2'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(2)\n",
    "combined['Rolling_Mean_4'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).mean()\n",
    "combined['Rolling_Std_4'] = combined.groupby(['Store', 'Dept'])['Weekly_Sales'].shift(1).rolling(window=4).std()\n",
    "\n",
    "# MarkDown total\n",
    "combined['MarkDown'] = combined['MarkDown1'] + combined['MarkDown2'] + combined['MarkDown4'] + combined['MarkDown5']\n",
    "\n",
    "# Drop unnecessary columns\n",
    "drop_cols = ['MarkDown1', 'MarkDown2', 'MarkDown4', 'MarkDown5', 'day_month_tuple', 'Date', 'source']\n",
    "combined.drop(columns=drop_cols, inplace=True)\n",
    "\n",
    "# Separate test portion\n",
    "test_final = combined[combined['Weekly_Sales'].isna()].copy()\n",
    "test_final.drop(columns=['Weekly_Sales'], inplace=True)\n",
    "\n",
    "# Drop columns as used during training\n",
    "X_test =test_final\n",
    "\n",
    "# Predict using the best XGBoost model\n",
    "xg__preds = xg1.predict(X_test[xg_feature_order])\n",
    "rf__preds= rf1.predict(X_test[rf_feature_order])\n",
    "lgb__preds=lgb1.predict(X_test[lgb_feature_order])\n",
    "final_preds=0.4*xg__preds+0.4*lgb__preds+0.2*rf__preds\n",
    "\n",
    "# Save results\n",
    "submission = pd.DataFrame({\n",
    "    'StoreId': test['Store'],\n",
    "    'Dept':test['Dept'],\n",
    "    'Date':test['Date'],\n",
    "    'Weekly_Sales': final_preds\n",
    "})\n",
    "submission.to_csv(\"submission.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ee826a-ee71-4a8d-a5da-dccd0ade7ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_feature_order = train_x.drop(xg_drop_columns,axis=1).columns.tolist()\n",
    "xg_feature_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17ced8fa-06b4-44f9-84ff-52e7448b77d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_feature_order = train_x.drop(rf_drop_columns,axis=1).columns.tolist()\n",
    "rf_feature_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1773ab64-8670-4e69-96ef-5db3b2a9bae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_feature_order = train_x.drop(lgb_drop_columns,axis=1).columns.tolist()\n",
    "lgb_feature_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c59988-65d8-4b0c-8578-6f7a839cfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=submission['Date'],y=submission['Weekly_Sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a465009-4871-4589-9b70-2f2d34914994",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=test_x['Day'],y=test_y,color='green')\n",
    "sns.scatterplot(x=test_x['Day'],y=rf1_preds,color='red',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763d5287-d1a2-4c4c-a831-8c2ae8636cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=test_x['Day'],y=test_y,color='green')\n",
    "sns.scatterplot(x=test_x['Day'],y=xg1_preds,color='red',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee8cf02-4b4e-4918-b2b0-0839e941ea3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=test_x['Day'],y=test_y,color='green')\n",
    "sns.scatterplot(x=test_x['Day'],y=lgb1_preds,color='red',alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0569920c-77fc-40b4-b769-5a93a7f584d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
